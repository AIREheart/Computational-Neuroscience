{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AIREheart/Computational-Neuroscience/blob/main/BertTakesaBoW/explore_input_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oP4vdU2D2P7"
      },
      "source": [
        "# Netherlands Neurogenetics Database\n",
        "Author: Nienke Mekkes <br>\n",
        "Date: 21-Sep-2022. <br>\n",
        "Correspond: n.j.mekkes@umcg.nl <br>\n",
        "\n",
        "## Script: clinical history labeled training data: cleaning & exploration\n",
        "Objectives: load and clean training data, do some basic data exploration\n",
        "\n",
        "\n",
        "### Input files:\n",
        "- excel file with labeled training data\n",
        "\n",
        "### Output:\n",
        "- excel file with cleaned labeled training data\n",
        "- pickle file with cleaned labeled training data\n",
        "- folder with figures with basic data explorations\n",
        "\n",
        "\n",
        "#### Minimal requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5GX3a4rD2QA"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75HdDnauD2QD",
        "outputId": "86a9fa40-5581-4efa-da82-dbfe1a9f1c1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.0.3)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
            "Collecting pywaffle\n",
            "  Downloading pywaffle-1.1.0-py2.py3-none-any.whl (30 kB)\n",
            "Collecting fontawesomefree (from pywaffle)\n",
            "  Downloading fontawesomefree-6.5.1-py3-none-any.whl (25.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.6/25.6 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from pywaffle) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pywaffle) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pywaffle) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pywaffle) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pywaffle) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pywaffle) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pywaffle) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pywaffle) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pywaffle) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pywaffle) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->pywaffle) (1.16.0)\n",
            "Installing collected packages: fontawesomefree, pywaffle\n",
            "Successfully installed fontawesomefree-6.5.1 pywaffle-1.1.0\n"
          ]
        }
      ],
      "source": [
        "%pip install pandas\n",
        "%pip install openpyxl\n",
        "%pip install seaborn\n",
        "%pip install pywaffle\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8O2yEMOD2QD"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksXcej2ND2QE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from pywaffle import Waffle\n",
        "from datetime import date\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0aB5tLiD2QE",
        "outputId": "97416e7c-f416-474f-bea3-13bff8465d41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "pd.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPXWJnguD2QF"
      },
      "source": [
        "#### Paths (user input required)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdvw0ZnZD2QG"
      },
      "outputs": [],
      "source": [
        "#path_to_training_data = r\"C:\\Users\\Mysti\\OneDrive\\Documents\\Model Inputs\\NLP_training_data.xlsx\"\n",
        "path_to_training_data = r\"C:/Users/Mysti/NLP_training_data.xlsx\"\n",
        "#/home/jupyter-n.mekkes@gmail.com-f6d87/clinical_history/input_data/Final_Labeling_300_testcasusen_inclusief_pilots_Megan.xlsx\"\n",
        "save_path_files = r\"C:/Users/Mysti\"\n",
        "save_path_figures = r\"C:/Users/Mysti\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gK2OmgZrD2QG",
        "outputId": "eef3992e-c350-462c-fa0d-8a9ecfe8491e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating figure folder....\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists(save_path_figures):\n",
        "    print('Creating figure folder....\\n')\n",
        "    os.makedirs(save_path_figures)\n",
        "\n",
        "if not os.path.exists(save_path_files):\n",
        "    print('Creating output folder....')\n",
        "    os.makedirs(save_path_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8T4dz6mD2QH"
      },
      "source": [
        "### Loading data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7b1KLCWD2QH"
      },
      "source": [
        "Training data comes in the form of an excel file with a tab per donor. <br>\n",
        "Merge all Excel sheets together with concat, keeping the sheet names. <br>\n",
        "The sheet names are the patient identifiers. <br>\n",
        "Note, empty sheets (== donors without clinical history) are ignored by concat <br>\n",
        "\n",
        "We also load a file with general informaiton about each donor, so we can add main diagnosis information <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ni29vJH5D2QI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "8b31055b-62da-4cf8-bee3-021244f6ad84"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'C:/Users/Mysti/NLP_training_data.xlsx'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-204d3aa1900b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Takes some time, so run only once. Rest of script functions on data copy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpd_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_training_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'openpyxl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mconcat_training_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m         raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1511\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1513\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1515\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_openpyxl.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer, storage_options)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \"\"\"\n\u001b[1;32m    548\u001b[0m         \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"openpyxl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer, storage_options)\u001b[0m\n\u001b[1;32m    528\u001b[0m         )\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mExcelFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workbook_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m    531\u001b[0m                 \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/Mysti/NLP_training_data.xlsx'"
          ]
        }
      ],
      "source": [
        "## Takes some time, so run only once. Rest of script functions on data copy.\n",
        "pd_df = pd.read_excel(path_to_training_data, engine='openpyxl', index_col=[0], sheet_name=None)\n",
        "concat_training_data = pd.concat(pd_df, axis=0, ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "data = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "pN1hIPG0mnUF",
        "outputId": "eef265be-c301-4d4c-9754-8ea031d484b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1dfe35d7-3749-442e-91ab-0ace9ac7d372\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1dfe35d7-3749-442e-91ab-0ace9ac7d372\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving NLP_training_data.xlsx to NLP_training_data.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Op8GW8rDD2QJ",
        "outputId": "ebabb4c8-1959-4d4f-d8c9-f974480d4247",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Training data has 18919 sentences and  1 columns. Non-attribute columns are:\n",
            "NBB_nr\n",
            "Year_Sentence_nr\n",
            "Sentence\n",
            "Index(['Unnamed: 1'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "pd_df = pd.read_excel(\"/content/NLP_training_data.xlsx\", engine='openpyxl', index_col=[0], sheet_name=0)\n",
        "training_data = pd_df\n",
        "\n",
        "print(type(training_data))\n",
        "## some attributes have very long names, these are shortened to prevent saving error\n",
        "training_data = training_data.rename(columns={\"Hyperreflexia_and_other_reflexes\":\"Hyperreflexia_and_oth_reflexes\",\n",
        "                                              \"Unspecified_disturbed_gait_patterns\": \"Unspecified_disturbed_gait_patt\",\n",
        "                                              \"Fatique\": \"Fatigue\",\n",
        "                                              \"Lack_of_planning_organisation_overview\":\"Lack_of_planning_organis_overv\"})\n",
        "\n",
        "print('Training data has', training_data.shape[0], 'sentences and ', training_data.shape[1], 'columns.' \\\n",
        "' Non-attribute columns are:')\n",
        "non_attribute_columns = ['NBB_nr','Year_Sentence_nr','Sentence']\n",
        "for i in non_attribute_columns: print(i)\n",
        "\n",
        "print(training_data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AObqIglDD2QK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "outputId": "2ed8e819-6630-4ea8-e4dd-5e5657d67a5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before combining donors, training data has  1 unique NBB identifiers.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'NBB_nr'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3653\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'NBB_nr'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-bd90a17ac352>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Before combining donors, training data has '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'unique NBB identifiers.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'After combining donors, training data has '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NBB_nr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'unique NBB identifiers.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbefore_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mafter_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NBB_nr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3761\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3762\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3763\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3655\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3656\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3657\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'NBB_nr'"
          ]
        }
      ],
      "source": [
        "print('Before combining donors, training data has ', len(pd_df.keys()), 'unique NBB identifiers.')\n",
        "print('After combining donors, training data has ',len(training_data['NBB_nr'].unique()),'unique NBB identifiers.')\n",
        "before_concat = list(pd_df.keys())\n",
        "after_concat = list(training_data['NBB_nr'].unique())\n",
        "\n",
        "print('Donor files without clinical history are:',list(np.setdiff1d(before_concat,after_concat)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbq76hi1D2QK"
      },
      "source": [
        "### Cleaning part 1\n",
        "-Remove NaN sentences (5) <br>\n",
        "-Remove sentences that are just a year (63) <br>\n",
        "-Make sure that all values are a boolean of either 1 or 0. <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hE4YfXIHD2QL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "7ba7a0e1-f22a-46b8-8d11-a7e9640d8693"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'dict' object has no attribute 'shape'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-1d6db8078668>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Before first round of cleaning, we have '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sentences.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m## remove NaN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'shape'"
          ]
        }
      ],
      "source": [
        "print('Before first round of cleaning, we have ',training_data.shape[0],'sentences.')\n",
        "\n",
        "## remove NaN\n",
        "training_data = training_data[training_data['Sentence'].notna()]\n",
        "\n",
        "## removing (year) sentences\n",
        "year = '\\(\\d+\\)$'\n",
        "training_data = training_data[lambda x: ~x['Sentence'].str.match(year)]\n",
        "\n",
        "## Shows the unique values in the 90 columns\n",
        "print('All values present in training data: ',\n",
        "      pd.unique(training_data.loc[:,[i for i in list(training_data.columns) if i not in non_attribute_columns]].values.ravel('K')))\n",
        "\n",
        "training_data = training_data.replace(\"TRUE \", True)\n",
        "training_data = training_data.replace(\"TRUE\", True)\n",
        "training_data = training_data.replace(\"True\", True)\n",
        "training_data = training_data.replace('False', False)\n",
        "training_data = training_data.replace(True, 1)\n",
        "training_data = training_data.replace(False, 0)\n",
        "print('All values present in training data after conversion: ',\n",
        "      pd.unique(training_data.iloc[:, 3:93].values.ravel('K')))\n",
        "\n",
        "print('After first round of cleaning, we have ',training_data.shape[0],'sentences.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1bcxBzcD2QL"
      },
      "source": [
        "### Cleaning part 2\n",
        "-Add non_attribute columns with Sentence length, number of scored attributes <br>\n",
        "-Remove sentences with more than 8 attributes <br>\n",
        "-Remove sentences with fewer than 6 characters <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ycbeiAtD2QM"
      },
      "outputs": [],
      "source": [
        "training_data = training_data[training_data.loc[:,[i for i in list(training_data.columns) if i not in non_attribute_columns]].sum(axis=1) < 9]\n",
        "training_data = training_data[training_data.Sentence.str.len() >= 6]\n",
        "training_data.loc[:,[i for i in list(training_data.columns) if i not in non_attribute_columns]].astype(int)\n",
        "# training_data =training_data[training_data['sentence_length'] <= 200]\n",
        "print('After second round of cleaning, we have ',training_data.shape[0],'sentences.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrK6SL5BD2QM"
      },
      "outputs": [],
      "source": [
        "training_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezpxmmmGD2QM"
      },
      "source": [
        "### additional sentences for relabeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27f5eBzuD2QN"
      },
      "outputs": [],
      "source": [
        "training_data_relabel = training_data[training_data.Sentence.str.len() >= 20]\n",
        "training_data_relabel = training_data_relabel.sample(n=1900, replace=False)\n",
        "training_data_relabel\n",
        "\n",
        "# since we cleaned up the training sentence file nicely, lets save it inbetween. this will be used to split the data.\n",
        "training_data_relabel.to_excel(f\"{save_path_files}/training_data_relabel.xlsx\")\n",
        "training_data_relabel.to_pickle(f\"{save_path_files}/training_data_relabel.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VowzAzMjD2QN"
      },
      "source": [
        "#### Should not be in final code, but this block creates supp. table 4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "IMgvYD3PD2QO"
      },
      "outputs": [],
      "source": [
        "comma_df = training_data#.drop(['sum_true','sentence_length'], axis=1)\n",
        "path_to_attribute_grouping = \"/home/jupyter-n.mekkes@gmail.com-f6d87/clinical_history/input_data/Clinical History - attributes grouping in categories - metadata.xlsx\"\n",
        "attribute_grouping = pd.read_excel(path_to_attribute_grouping, engine='openpyxl', index_col=[0], sheet_name='90 parameters')\n",
        "correct_names = {}\n",
        "for attr, real_name in zip(attribute_grouping.index, attribute_grouping[\"Attribute\"]):\n",
        "    if not isinstance(real_name, float):\n",
        "        correct_names[real_name] = attr\n",
        "# print(correct_names)\n",
        "comma_df = comma_df.rename(correct_names,axis=1)\n",
        "comma_df.loc[:, 'Muscular weakness':'Admission to nursing home'] = comma_df.loc[:, 'Muscular weakness':'Admission to nursing home'].replace(0, np.nan)\n",
        "comma_df.loc[:, 'Muscular weakness':'Admission to nursing home'] = comma_df.loc[:, 'Muscular weakness':'Admission to nursing home'].replace(1, pd.Series(comma_df.columns, comma_df.columns))\n",
        "comma_df['Attribute(s)'] = comma_df.loc[:, 'Muscular weakness':'Admission to nursing home'].apply(lambda x: ','.join(x[x.notnull()]), axis = 1)\n",
        "comma_df = comma_df[['NBB_nr','Sentence','Attribute(s)']]\n",
        "display(comma_df.head(20))\n",
        "comma_df.to_excel(f\"{save_path_files}/sup4_chrono{date.today()}.xlsx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TLhu1PLD2QO"
      },
      "source": [
        "#### Save as cleaned training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMUn12NnD2QO"
      },
      "outputs": [],
      "source": [
        "# since we cleaned up the training sentence file nicely, lets save it inbetween. this will be used to split the data.\n",
        "training_data.to_excel(f\"{save_path_files}/cleaned_training_data.xlsx\")\n",
        "training_data.to_pickle(f\"{save_path_files}/cleaned_training_data.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IGHIoAVD2QP"
      },
      "outputs": [],
      "source": [
        "## dotplot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5RwbPQCD2QP"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_rows', 90)\n",
        "frequency = training_data.copy()\n",
        "frequency = frequency.iloc[:, -90:].sum()\n",
        "print(frequency.sort_values())\n",
        "# frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKcCF85hD2QP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxzzAf1fD2QQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sz86f2XhD2QQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwJOlWoWD2QQ"
      },
      "source": [
        "### DATA EXPLORATION (optional)\n",
        "#### How long are our training sentences?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMJeFZ-9D2QQ"
      },
      "outputs": [],
      "source": [
        "df = training_data_relabel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fu8qtVBBD2QR"
      },
      "outputs": [],
      "source": [
        "df[\"sum_true\"] = df.loc[:,[i for i in list(df.columns) if i not in non_attribute_columns]].sum(axis=1)\n",
        "df['sentence_length'] = df.Sentence.str.len()\n",
        "\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "sns.set(rc={'figure.figsize':(15,6)},font_scale = 2) #\n",
        "sns.set_palette(\"pastel\")\n",
        "sns.set_style(\"ticks\")\n",
        "\n",
        "## get the frequency of the previously created column sentence_length\n",
        "length_distribution = pd.DataFrame(df['sentence_length'].value_counts())\n",
        "length_distribution['x'] = length_distribution.index\n",
        "length_distribution.columns = ['nr_sentences','sentence_length']\n",
        "zero_row = {'nr_sentences':0, 'sentence_length':0}\n",
        "# length_distribution = length_distribution.append(zero_row, ignore_index=True)\n",
        "length_distribution = pd.concat([length_distribution, pd.DataFrame.from_records([zero_row])],\n",
        "                                ignore_index=True)\n",
        "length_distribution = length_distribution.sort_values(by=['sentence_length'])\n",
        "\n",
        "## plot and save\n",
        "lh = sns.barplot(x=\"sentence_length\", y=\"nr_sentences\", data=length_distribution,color='steelblue')\n",
        "lh.set(xlabel=\"Sentence length\", ylabel=\"Number of sentences\")\n",
        "plt.title(\"Sentence length distribution -- Training data\", y=1.1, fontsize = 20)\n",
        "plt.xticks(rotation=90)\n",
        "for ind, label in enumerate(lh.get_xticklabels()):\n",
        "    if ind == 0:\n",
        "        label.set_visible(True)\n",
        "    elif ind % 10 == 0:\n",
        "        label.set_visible(True)\n",
        "    else:\n",
        "        label.set_visible(False)\n",
        "\n",
        "sns.despine(offset=10, trim=False)\n",
        "lh.spines[\"right\"].set_color(\"none\")\n",
        "lh.spines[\"top\"].set_color(\"none\")\n",
        "\n",
        "# plt.savefig(save_path_figures + \"/training_data_sentence_length_distribution_{}.png\".format(date.today()),\n",
        "#             dpi=600, bbox_inches=\"tight\")\n",
        "# plt.savefig(save_path_figures + \"/training_data_sentence_length_distribution_{}.pdf\".format(date.today()),\n",
        "#             dpi=600, bbox_inches=\"tight\")\n",
        "# plt.show()\n",
        "# plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuFTLUTjD2QR"
      },
      "source": [
        "### How many sentences have how many attributes?\n",
        "We expect that most sentences have no attribute, many sentences will have a single attribute, and a high amount of attributes for a single sentence is unlikely"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7jEIGgRD2QS"
      },
      "outputs": [],
      "source": [
        "\n",
        "## get the frequency of the previously created column counting attributes per sentence\n",
        "attribute_distribution = pd.DataFrame(df['sum_true'].value_counts())\n",
        "attribute_distribution['nr_attributes'] = attribute_distribution.index\n",
        "attribute_distribution.columns = ['nr_sentences','nr_attributes']\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,6))\n",
        "ax =  sns.barplot(x=\"nr_attributes\", y=\"nr_sentences\", data=attribute_distribution,color = 'steelblue')\n",
        "sns.despine(offset=10, trim=False)\n",
        "ax.spines[\"right\"].set_color(\"none\")\n",
        "ax.spines[\"top\"].set_color(\"none\")\n",
        "\n",
        "ax.set_xlabel(\"Number of attributes per sentence\",fontsize=20)\n",
        "ax.set_ylabel(\"Sentence count\",fontsize=25)\n",
        "ax.tick_params(labelsize=20)\n",
        "# plt.savefig(save_path_figures + \"/training_data_sentence_att_{}.pdf\".format(date.today()),\n",
        "#             bbox_inches=\"tight\",dpi=600)\n",
        "# plt.savefig(save_path_figures + \"/training_data_sentence_att_{}.png\".format(date.today()),\n",
        "#             bbox_inches=\"tight\",dpi=600)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnXt92YED2QS"
      },
      "source": [
        "##### More intuitive is to plot as a waffle:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeTXNZvaD2QT"
      },
      "outputs": [],
      "source": [
        "attribute_distribution['proportion'] = round(attribute_distribution['nr_sentences']/attribute_distribution['nr_sentences'].sum()*100,2)\n",
        "palette = sns.color_palette(\"tab20\")[0:8]\n",
        "attribute_distribution['legend'] = attribute_distribution['nr_attributes'].astype(str) +': ' + attribute_distribution['proportion'].astype(str) + '%'\n",
        "test = pd.Series(attribute_distribution.nr_sentences.values,index=attribute_distribution.nr_attributes).to_dict()\n",
        "\n",
        "fig = plt.figure(\n",
        "    FigureClass=Waffle,\n",
        "    rows=30,\n",
        "    values=list(attribute_distribution.nr_sentences/13),\n",
        "    colors = palette,\n",
        "    figsize=(10, 8),\n",
        "        legend={'labels':list(attribute_distribution.legend),\n",
        "            'loc': 'upper right', 'bbox_to_anchor': (1.1, -0.07),\n",
        "            'ncol': 5,\n",
        "            'framealpha': 0,\n",
        "            'title':'Attributes per sentence',\n",
        "            'title_fontsize':8,\n",
        "            'fontsize': 6\n",
        "               }\n",
        ")\n",
        "# plt.savefig(save_path_figures + \"/training_data_waffle_sentence_att_{}.pdf\".format(date.today()),\n",
        "#             bbox_inches=\"tight\",dpi=600)\n",
        "# plt.savefig(save_path_figures + \"/training_data_waffle_sentence_att_{}.png\".format(date.today()),\n",
        "#             bbox_inches=\"tight\",dpi=600)\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4urzdmpoD2QT"
      },
      "source": [
        "### What is the relationship between sentence length and number of attributes?\n",
        "We expect that longer sentences have more attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E46Jz7NsD2QT"
      },
      "outputs": [],
      "source": [
        "g = sns.catplot(x='sum_true', y='sentence_length', data=df,kind=\"violin\",inner=None, palette='Blues')\n",
        "plt.xlabel(\"Number of attributes\")\n",
        "plt.ylabel(\"Sentence length\")\n",
        "plt.tick_params(labelsize=10)\n",
        "# plt.savefig(save_path_figures + \"/training_data_violin_sentence_length_attributes_{}.png\".format(date.today()),\n",
        "#             bbox_inches=\"tight\",dpi=600)\n",
        "# plt.savefig(save_path_figures + \"/training_data_violin_sentence_length_attributes_{}.pdf\".format(date.today()),\n",
        "#             bbox_inches=\"tight\",dpi=600)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20vLrQTRD2Qd"
      },
      "source": [
        "### Training set: sentence distribution\n",
        "We expect to find differences in how many sentences each donor has"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FD-ua71D2Qe"
      },
      "outputs": [],
      "source": [
        "%config InlineBackend.figure_format = 'svg'\n",
        "sns.set(rc={'figure.figsize':(30,6)},font_scale = 2) #\n",
        "sns.set_palette(\"pastel\")\n",
        "sns.set_style(\"ticks\")\n",
        "\n",
        "## NBB nr frequency df\n",
        "sentences_per_donor = pd.DataFrame(df['NBB_nr'].value_counts())\n",
        "sentences_per_donor['x'] = sentences_per_donor.index\n",
        "sentences_per_donor.columns = ['nr_sentences','NBB_nr']\n",
        "\n",
        "## plot\n",
        "ax = sns.barplot(x=\"NBB_nr\", y=\"nr_sentences\", data=sentences_per_donor, color='steelblue')\n",
        "plt.xlabel(\"Donor ID\")\n",
        "plt.ylabel(\"# of sentences\")\n",
        "\n",
        "plt.tick_params(labelsize=8)\n",
        "ax.spines[\"right\"].set_color(\"none\")\n",
        "ax.spines[\"top\"].set_color(\"none\")\n",
        "plt.xticks(rotation=90)\n",
        "# plt.savefig(save_path_figures + \"/training_data_sentences_per_donor_{}.png\".format(date.today()),\n",
        "#             bbox_inches=\"tight\",dpi=600)\n",
        "# plt.savefig(save_path_figures + \"/training_data_sentences_per_donor_{}.pdf\".format(date.today()),\n",
        "#             bbox_inches=\"tight\",dpi=600)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRWKkXPfD2Qf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVCTjRs9D2Qf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "machine_learning_env",
      "language": "python",
      "name": "machine_learning_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}